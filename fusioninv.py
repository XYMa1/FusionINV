import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

import sys
from typing import List

import numpy as np
import pyrallis
import torch
from PIL import Image


def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    import random
    random.seed(seed)


sys.path.append(".")
sys.path.append(". .")

from AllinVIS import AllinVISModel
from config import RunConfig, Range
from utils import latent_utils
from utils. latent_utils import load_latents_or_invert_images


@pyrallis.wrap()
def main(cfg: RunConfig):
    run(cfg)


# ========== æ–°å¢ï¼šæ— å‚æ•°è¿è¡Œå…¥å£ ==========
def main_with_defaults():
    """
    ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œï¼ˆæ–¹ä¾¿PyCharmç›´æ¥è¿è¡Œï¼‰
    """
    from pathlib import Path

    # è®¾ç½®é»˜è®¤å‚æ•°
    cfg = RunConfig(
        vis_image_path=Path("data/LLVIP/vi/1.jpg"),
        ir_image_path=Path("data/LLVIP/ir/1.jpg"),
        domain_name="LLVIP",
        num_timesteps=100,
        load_latents=False,  # é¦–æ¬¡è¿è¡Œè®¾ä¸ºFalseï¼Œä¹‹åæ”¹ä¸ºTrueåŠ é€Ÿ
        skip_steps=25,
        seed=42,
    )

    print("=" * 60)
    print("ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œ LIT-Fusion")
    print("=" * 60)
    print(f"  å¯è§å…‰å›¾åƒ:  {cfg.vis_image_path}")
    print(f"  çº¢å¤–å›¾åƒ:     {cfg.ir_image_path}")
    print(f"  å»å™ªæ­¥æ•°:   {cfg.num_timesteps}")
    print(f"  åŸŸåç§°:     {cfg.domain_name}")
    print("=" * 60)

    run(cfg)


# =========================================


def run(cfg: RunConfig) -> List[Image.Image]:
    """
    æ··åˆèåˆä¸»æµç¨‹ï¼šä¼ ç»Ÿèåˆ + SDèåˆçš„è½¯åŠ æƒ
    """
    import numpy as np
    from utils.exposure_metrics import compute_exposure
    from utils.image_utils import load_images
    from utils.hybrid_fusion import (
        traditional_fusion,
        compute_confidence_weight,
        blend_images,
        adaptive_config_for_exposure,
        print_fusion_strategy
    )

    pyrallis.dump(cfg, open(cfg.output_path / 'config.yaml', 'w'))
    set_seed(cfg.seed)

    # ========== é˜¶æ®µ1ï¼šåŠ è½½å›¾åƒå¹¶è®¡ç®—æ›å…‰åº¦ ==========
    print("ğŸ“¥ åŠ è½½å›¾åƒ...")
    vis_img, ir_img = load_images(cfg=cfg, save_path=cfg.output_path)
    vis_np = np.array(vis_img)
    E_vi = compute_exposure(vis_np)

    # ========== é˜¶æ®µ2ï¼šè®¡ç®—ç½®ä¿¡åº¦æƒé‡ ==========
    alpha = compute_confidence_weight(E_vi, center=0.25, smooth=0.1)

    # ========== é˜¶æ®µ3ï¼šè‡ªé€‚åº”å‚æ•°è°ƒæ•´ ==========
    params = adaptive_config_for_exposure(E_vi, cfg)
    print_fusion_strategy(E_vi, alpha, params)

    # ========== é˜¶æ®µ4ï¼šä¿å®ˆåˆ†æ”¯ï¼ˆä¼ ç»Ÿèåˆï¼‰==========
    print("ğŸ”§ [ä¿å®ˆåˆ†æ”¯] æ‰§è¡Œä¼ ç»ŸåŠ æƒèåˆ...")
    # åŸºäºé¥±å’Œåº¦çš„è‰²å½©ä¿ç•™ + å¤šçº§äº®åº¦å¢å¼º
    # VIä¸»å¯¼èåˆ + è‰²å½©å¢å¼º + ç»†èŠ‚ä¿ç•™
    # ç®€åŒ–çš„ä¼ ç»Ÿèåˆï¼ˆä½œä¸ºSDçš„åŸºç¡€ï¼‰
    I_trad = traditional_fusion(vis_img, ir_img,
                                w_vi=0.6,  # å¹³è¡¡
                                w_ir=0.6,  # å¹³è¡¡
                                gamma=0.65,  # æäº®
                                saturation_boost=1.2)  # æ¸©å’Œå¢å¼º

    # ä¿å­˜ä¼ ç»Ÿèåˆç»“æœï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    base_name = cfg.ir_image_path.stem
    I_trad.save(cfg.output_path / f"{base_name}_traditional.png")
    print(f"   âœ… ä¼ ç»Ÿèåˆå®Œæˆï¼Œå·²ä¿å­˜ä¸º:  {base_name}_traditional.png")

    # ========== é˜¶æ®µ5ï¼šç”Ÿæˆåˆ†æ”¯ï¼ˆSDèåˆï¼‰==========
    if params['run_sd']:
        print(f"ğŸš€ [ç”Ÿæˆåˆ†æ”¯] æ‰§è¡ŒSDèåˆ (skip={params['skip_steps']}, CFG={params['swap_guidance_scale']})...")

        # åº”ç”¨è‡ªé€‚åº”å‚æ•°
        cfg.skip_steps = params['skip_steps']
        cfg.swap_guidance_scale = params['swap_guidance_scale']
        cfg.E_vi = E_vi

        # è¿è¡ŒSDèåˆ
        model = AllinVISModel(cfg)
        latents_vis, latents_ir, noise_vis, noise_ir = load_latents_or_invert_images(model=model, cfg=cfg)
        model.set_latents(latents_vis, latents_ir)
        model.set_noise(noise_vis, noise_ir)

        if hasattr(cfg, 'E_vi'):
            model.E_vi = cfg.E_vi

        images_sd = run_infraredvisiblefusion(model=model, cfg=cfg)
        I_sd = images_sd[0]  # èåˆç»“æœ

        # ä¿å­˜SDèåˆç»“æœï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        I_sd.save(cfg.output_path / f"{base_name}_sd.png")
        print(f"   âœ… SDèåˆå®Œæˆï¼Œå·²ä¿å­˜ä¸º: {base_name}_sd.png")

    else:
        # ææš—åœºæ™¯ï¼šè·³è¿‡SDï¼ŒèŠ‚çœæ—¶é—´
        print("âš¡ [ç”Ÿæˆåˆ†æ”¯] åœºæ™¯ææš—ï¼Œè·³è¿‡SDèåˆï¼ˆèŠ‚çœè®¡ç®—èµ„æºï¼‰")
        I_sd = I_trad  # ä½¿ç”¨ä¼ ç»Ÿèåˆä½œä¸ºæ›¿ä»£

    # ========== é˜¶æ®µ6ï¼šç½®ä¿¡åº¦åŠ æƒæ··åˆ ==========
    print(f"âš–ï¸  [æ··åˆ] ç½®ä¿¡åº¦åŠ æƒæ··åˆ (Î±={alpha:.3f})...")
    I_final = blend_images(I_sd, I_trad, alpha)

    # ========== é˜¶æ®µ7ï¼šä¿å­˜ç»“æœ ==========
    I_final.save(cfg.output_path / f"{base_name}.png")

    # è½¬æ¢numpyæ•°ç»„åˆ°PIL Imageï¼ˆå¦‚æœéœ€è¦ï¼‰
    if isinstance(vis_img, np.ndarray):
        vis_img = Image.fromarray(vis_img)
    if isinstance(ir_img, np.ndarray):
        ir_img = Image.fromarray(ir_img)

    vis_img.save(cfg.output_path / f"out_vis_{base_name}.png")
    ir_img.save(cfg.output_path / f"out_ir_{base_name}.png")

    print("\n" + "=" * 70)
    print("âœ… æ··åˆèåˆå®Œæˆï¼")
    print("=" * 70)
    print(f"  ğŸ“ è¾“å‡ºç›®å½•:  {cfg.output_path}")
    print(f"  ğŸ“„ æœ€ç»ˆç»“æœ:  {base_name}.png (æ··åˆ)")
    print(f"  ğŸ“„ å¯¹æ¯”ç»“æœ:")
    print(f"      â†’ {base_name}_traditional.png (ä¼ ç»Ÿèåˆï¼ŒÎ±=0)")
    if params['run_sd']:
        print(f"      â†’ {base_name}_sd.png (SDèåˆï¼ŒÎ±=1)")
    print(f"  ğŸ“Š æ··åˆæƒé‡: {alpha * 100:.1f}% SD + {(1 - alpha) * 100:.1f}% ä¼ ç»Ÿ")
    print("=" * 70 + "\n")

    return [I_final, vis_img, ir_img]


def run_infraredvisiblefusion(model: AllinVISModel, cfg: RunConfig) -> List[Image.Image]:
    init_latents, init_zs = latent_utils.get_init_latents_and_noises(model=model, cfg=cfg)
    model.pipe. scheduler.set_timesteps(cfg. num_timesteps)
    model.enable_edit = True  # Activate our cross-image attention layers
    start_step = min(cfg.cross_attn_32_range.start, cfg.cross_attn_64_range.start)
    end_step = max(cfg.cross_attn_32_range. end, cfg.cross_attn_64_range.end)

    # ========== å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ä½¿ç”¨ä¸­æ€§æç¤ºè¯ ==========
    print("  [è°ƒè¯•] ä½¿ç”¨ä¸­æ€§æç¤ºè¯:  'a photo' x3")
    visir_prompts = ["a photo", "a photo", "a photo"]
    # ==============================================

    images = model.pipe(
        prompt=visir_prompts,
        latents=init_latents,
        guidance_scale=cfg.swap_guidance_scale,
        num_inference_steps=cfg.num_timesteps,
        swap_guidance_scale=cfg.swap_guidance_scale,
        callback=model.get_adain_callback(),
        eta=1,
        zs=init_zs,
        generator=torch.Generator('cuda').manual_seed(cfg.seed),
        cross_image_attention_range=Range(start=start_step, end=end_step),
    ).images

    # Save images
    # ========== ä¿®æ”¹ï¼šä½¿ç”¨è¾“å…¥æ–‡ä»¶å ==========
    base_name = cfg.ir_image_path.stem  # ä½¿ç”¨IRå›¾åƒçš„æ–‡ä»¶åï¼ˆå¦‚ "1"ï¼‰

    # Save images
    images[0].save(cfg.output_path / f"{base_name}.png")  # èåˆç»“æœï¼š1.png
    images[1]. save(cfg.output_path / f"out_vis_{base_name}.png")  # out_vis_1.png
    images[2].save(cfg. output_path / f"out_ir_{base_name}.png")  # out_ir_1.png

    print(f"\nâœ… èåˆå®Œæˆï¼Œç»“æœå·²ä¿å­˜:")
    print(f"  èåˆ:    {cfg.output_path}/{base_name}.png")
    print(f"  å¯è§å…‰:   {cfg.output_path}/out_vis_{base_name}.png")
    print(f"  çº¢å¤–:   {cfg.output_path}/out_ir_{base_name}.png")
    # =========================================

    return images


if __name__ == '__main__':
    import sys

    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    if len(sys.argv) == 1:
        print("âš¡ æ£€æµ‹åˆ°æ— å‚æ•°è¿è¡Œï¼Œä½¿ç”¨é»˜è®¤é…ç½®...")
        main_with_defaults()
    else:
        # æœ‰å‚æ•°æ—¶ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
        main()

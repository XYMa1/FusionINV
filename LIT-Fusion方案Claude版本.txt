LIT-Fusion (Semantically-Guided Low-Light Infrared and Visible Image Fusion via Adaptive Latent Inversion)
1ã€æ ¸å¿ƒåˆ›æ–°ç‚¹

åˆ›æ–°ç‚¹ 1ï¼šç…§æ˜æ„ŸçŸ¥çš„åŒè·¯æ½œå±‚åˆå§‹åŒ–

Illumination-Aware Dual-Stream Latent Initialization

ç—›ç‚¹ï¼š

ç›´æ¥å¯¹æä½å…‰ç…§å›¾åƒè¿›è¡Œ DDPM Inversion ä¼šå¯¼è‡´æ½œåœ¨ç¼–ç  (Latent Code) é™·å…¥"é»‘æš—æµå½¢"ï¼Œä¸¢å¤±è¯­ä¹‰ä¿¡æ¯ï¼Œæ— æ³•ä¸ºèåˆæä¾›æœ‰æ•ˆå¼•å¯¼ã€‚

åˆ›æ–°ï¼š

æå‡º Retinex è§£è€¦ä¸è¯­ä¹‰å¼•å¯¼åæ¼”ç›¸ç»“åˆçš„åŒè·¯åˆå§‹åŒ–ç­–ç•¥ï¼š

çº¢å¤–è·¯ï¼šæå–å‡ ä½•è¾¹ç¼˜ä½œä¸ºç»“æ„çº¦æŸ

å¯è§å…‰è·¯ï¼šåˆ†ç¦»åå°„åˆ†é‡ä¸å…‰ç…§åˆ†é‡ï¼Œä»…å¢å¼ºå…‰ç…§ä¿æŒè‰²å½©ä¿çœŸï¼Œç»“åˆç…§æ˜æ„ŸçŸ¥æç¤ºè¯è¿›è¡Œå®šå‘åæ¼”

å­¦æœ¯ä»·å€¼ï¼š

æ‰“ç ´ä¼ ç»Ÿæ–¹æ³•å¯¹è¾“å…¥å›¾åƒè´¨é‡çš„ä¾èµ–ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå…ˆéªŒé‡å»ºä¸¢å¤±çš„å…‰ç…§ä¿¡æ¯ã€‚

åˆ›æ–°ç‚¹ 2ï¼šç»“æ„çº¦æŸçš„ä¸‰æµæ··åˆæ³¨æ„åŠ›æœºåˆ¶

Structure-Constrained Triple-Stream Hybrid Attention

ç—›ç‚¹ï¼š

ç°æœ‰èåˆæ–¹æ³•éš¾ä»¥å¹³è¡¡çº¢å¤–çš„ç»“æ„ä¿çœŸã€å¯è§å…‰çš„è¯­ä¹‰ä¸€è‡´æ€§ã€ä»¥åŠä½å…‰åœºæ™¯çš„å…‰ç…§é‡å»ºä¸‰è€…å…³ç³»ã€‚

åˆ›æ–°ï¼š

åœ¨ U-Net è§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å±‚ä¸­ï¼ŒåŠ¨æ€æ··åˆä¸‰ä¸ªä¿¡æ¯æºï¼š

ç»“æ„æµ (Kir,Vir)ï¼šçº¢å¤–çƒ­ç›®æ ‡çš„å‡ ä½•ç‰¹å¾

è¯­ä¹‰æµ (Kvi,Vvi)ï¼šå¯è§å…‰çš„ç‰©ä½“ç±»åˆ«ä¿¡æ¯

å…‰ç…§æµ (Ktxt,Vtxt)ï¼šæ–‡æœ¬æç¤ºçš„å…‰ç…§é£æ ¼å…ˆéªŒ

é€šè¿‡è‡ªé€‚åº”æƒé‡ w1(t),w2(t),w3(t) åœ¨å»å™ªè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒåº¦ä¸‰è€…è´¡çŒ®ã€‚

å­¦æœ¯ä»·å€¼ï¼š

é¦–æ¬¡åœ¨æ‰©æ•£æ¨¡å‹æ¡†æ¶ä¸‹å®ç°å¤šæ¨¡æ€ç‰¹å¾çš„è§£è€¦ä¸å¯æ§èåˆï¼Œæ— éœ€è®­ç»ƒå³å¯é€‚åº”ä¸åŒå…‰ç…§æ¡ä»¶ã€‚

åˆ›æ–°ç‚¹ 3ï¼šæ›å…‰æ„ŸçŸ¥çš„è‡ªé€‚åº”æ—¶åºè°ƒåº¦

Exposure-Aware Adaptive Temporal Scheduling

ç—›ç‚¹ï¼š

å›ºå®šçš„èåˆæ—¶åºï¼ˆå¦‚å‰ 40 æ­¥ç”¨ IRï¼Œå 60 æ­¥ç”¨ VIï¼‰æ— æ³•åº”å¯¹ææš—ã€å¼±å…‰ã€æ­£å¸¸å…‰ç­‰ä¸åŒåœºæ™¯ã€‚

åˆ›æ–°ï¼š

åŸºäºè¾“å…¥å›¾åƒçš„æ›å…‰åº¦æŒ‡æ ‡ Evi åŠ¨æ€è°ƒæ•´ä¸‰æµæƒé‡  æ—¶åºæ¼”åŒ–æ›²çº¿ï¼š

ææš—åœºæ™¯ï¼šå…¨ç¨‹å¼ºåŒ–å…‰ç…§æµå’Œç»“æ„æµ

å¼±å…‰åœºæ™¯ï¼šä¸­æœŸå³å¼€å§‹å¼•å…¥è¯­ä¹‰æµ

æ­£å¸¸å…‰ï¼šå¿«é€Ÿè¿‡æ¸¡åˆ°è¯­ä¹‰æµä¸»å¯¼

å­¦æœ¯ä»·å€¼ï¼š

æå‡ºé¦–ä¸ªè‡ªé€‚åº”çš„å¤šæ¨¡æ€èåˆè°ƒåº¦ç­–ç•¥ï¼Œå®ç°"ä¸€ä¸ªæ¨¡å‹ï¼Œå…¨åœºæ™¯é€‚é…"ã€‚

2ã€å…·ä½“å®ç°è·¯å¾„ï¼ˆRefined Pipelineï¼‰

é˜¶æ®µ 1ï¼šç…§æ˜æ„ŸçŸ¥çš„åŒè·¯æ½œå±‚åˆå§‹åŒ–

1.1 çº¢å¤–è·¯ (IR Stream)

è¾“å…¥ï¼šI_ir âˆˆ â„^(HÃ—WÃ—3)

  â†“

è¾¹ç¼˜æå–ï¼šE_ir = Canny(RGB2Gray(I_ir), threshold=[50,150])

  â†“

ç‰¹å¾ç¼–ç ï¼šz_ir = VAE_encode(I_ir)  # [1, 4, 64, 64]

  â†“

DDPM Inversionï¼š

  - Prompt: "" (ç©ºæç¤ºï¼Œä¿ç•™åŸå§‹çƒ­ç‰¹å¾)

  - CFG Scale: 1.0 (æ— å¼•å¯¼)

  â†’ noise_ir, latents_ir

1.2 å¯è§å…‰è·¯ (VI Stream - æ ¸å¿ƒæ”¹è¿›)

è¾“å…¥ï¼šI_vi âˆˆ â„^(HÃ—WÃ—3) (ä½å…‰å›¾åƒ)

  â†“

ã€æ­¥éª¤1: Retinex åˆ†è§£ã€‘

  R, L = Retinex_decompose(I_vi)

  # R: åå°„åˆ†é‡ï¼ˆç‰©ä½“æœ¬è‰²ï¼‰

  # L: å…‰ç…§åˆ†é‡ï¼ˆäº®åº¦ï¼‰

  â†“

ã€æ­¥éª¤2: ç…§æ˜å¢å¼ºã€‘

  L_enhanced = L^Î³, where Î³ = 0.4 (ææš—) or 0.5 (å¼±å…‰)

  I_vi_enhanced = R âŠ™ L_enhanced  # é€å…ƒç´ ä¹˜æ³•

  â†“

ã€æ­¥éª¤3: è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡ã€‘

  I_vi_pre = CLAHE(I_vi_enhanced, clip_limit=0.03)

  â†“

ã€æ­¥éª¤4: æ›å…‰åº¦è¯„ä¼°ã€‘

  I_gray = 0.299*R + 0.587*G + 0.114*B

  E_vi = mean(I_gray) / 255.0  # âˆˆ [0, 1]

  â†“

ã€æ­¥éª¤5: åŠ¨æ€æç¤ºè¯ç”Ÿæˆã€‘

  if E_vi < 0.2: 

      prompt = "extremely dark scene transformed to bright daylight"

  elif E_vi < 0.4:

      prompt = "low light scene with natural illumination"

  else:

      prompt = "dimly lit scene with enhanced visibility"

  â†“

ã€æ­¥éª¤6: å¼•å¯¼åæ¼”ã€‘

  noise_vi, latents_vi = DDPM_Inversion(

      x0 = I_vi_pre,

      prompt = prompt,

      cfg_scale = 5.5,

      num_steps = 100

  )

é˜¶æ®µ 2ï¼šç»“æ„çº¦æŸçš„ä¸‰æµæ··åˆæ³¨æ„åŠ›æœºåˆ¶

2.1 å‡†å¤‡å·¥ä½œ

# åˆå§‹åŒ–ä¸‰è·¯æ½œåœ¨ç¼–ç 

z_fusion = z_vi. clone()  # ä»å¯è§å…‰åˆå§‹åŒ–

z_t = [z_fusion, z_vi, z_ir]  # 3ä¸ªæ ·æœ¬æ‰¹æ¬¡



# ControlNet é¢„å¤„ç†ï¼ˆä½æƒé‡ï¼ŒåæœŸå¯è°ƒï¼‰

controlnet_weight = 0.15  # åˆå§‹å€¼å¯è®¾ä¸º 0.0 æµ‹è¯•

edge_condition = preprocess_edge(E_ir)

2.2 å»å™ªå¾ªç¯æ ¸å¿ƒï¼ˆåœ¨ U-Net Decoder çš„ Self-Attention å±‚ï¼‰

class TripleStreamAttentionProcessor: 

    def __call__(self, attn, hidden_states, ... ):

        # æ ‡å‡† Attention è®¡ç®—

        Q = attn.to_q(hidden_states)

        K_original = attn.to_k(hidden_states)

        V_original = attn.to_v(hidden_states)

        

        # æå–ä¸‰æµç‰¹å¾

        K_ir, V_ir = K_original[IR_INDEX], V_original[IR_INDEX]

        K_vi, V_vi = K_original[VI_INDEX], V_original[VI_INDEX]

        

        # ğŸ”‘ ä» Cross-Attention çªƒå–æ–‡æœ¬ç‰¹å¾

        K_txt, V_txt = self.get_text_features_from_cross_attn()

        

        # ğŸ¯ è®¡ç®—è‡ªé€‚åº”æƒé‡

        w1, w2, w3 = compute_adaptive_weights(

            timestep=self.current_step,

            exposure=self.E_vi,

            total_steps=self.total_steps

        )

        

        # ğŸ”€ ä¸‰æµèåˆ

        K_fused = w1 * K_ir + w2 * K_vi + w3 * K_txt

        V_fused = w1 * V_ir + w2 * V_vi + w3 * V_txt

        

        # ğŸ”§ ControlNet å¢å¼ºï¼ˆå¯é€‰ï¼Œæƒé‡å¯ä¸º0ï¼‰

        if self.use_controlnet:

            ctrl_features = self.controlnet_residual

            K_fused = K_fused + controlnet_weight * ctrl_features['K']

            V_fused = V_fused + controlnet_weight * ctrl_features['V']

        

        # è®¡ç®— Attention

        K_fused[OUT_INDEX] = K_fused

        V_fused[OUT_INDEX] = V_fused

        

        attn_output = scaled_dot_product_attention(Q, K_fused, V_fused)

        return attn_output

2.3 æ–‡æœ¬ç‰¹å¾æå–ï¼ˆå…³é”®å®ç°ï¼‰

def get_text_features_from_cross_attn(self):

    """ä» Cross-Attention å±‚çªƒå–å·²å¤„ç†çš„æ–‡æœ¬ç‰¹å¾"""

    # åœ¨ Cross-Attention forward æ—¶ä¿å­˜

    # text_embeds: [batch, 77, 768] æ¥è‡ª CLIP

    # ç»è¿‡ to_k, to_v æŠ•å½±å:  [batch, seq_len, dim]

    

    K_txt = self.cached_cross_attn_K  # ç”±å¤–éƒ¨æ³¨å…¥

    V_txt = self.cached_cross_attn_V

    

    # å¹³å‡æ± åŒ–åˆ° Self-Attention çš„åºåˆ—é•¿åº¦

    K_txt = adaptive_pool(K_txt, target_len=self.spatial_res**2)

    V_txt = adaptive_pool(V_txt, target_len=self.spatial_res**2)

    

    return K_txt, V_txt

é˜¶æ®µ 3ï¼šæ›å…‰æ„ŸçŸ¥çš„è‡ªé€‚åº”æ—¶åºè°ƒåº¦

3.1 æƒé‡è®¡ç®—å‡½æ•°ï¼ˆä»£ç å®ç° - çº¿æ€§ç‰ˆ MVPï¼‰

def compute_adaptive_weights_linear(t, T, E_vi):

    """

    Args:

        t: å½“å‰æ—¶é—´æ­¥ (100 â†’ 0)

        T: æ€»æ—¶é—´æ­¥æ•° (100)

        E_vi: æ›å…‰åº¦ [0, 1]

    Returns:

        w1, w2, w3: å½’ä¸€åŒ–æƒé‡

    """

    t_norm = t / T  # å½’ä¸€åŒ–åˆ° [1, 0]

    

    # é˜¶æ®µåˆ¤æ–­

    if t_norm > 0.6:  # Early:  å¼ºç»“æ„

        w1_base = 0.7

        w2_base = 0.2

        w3_base = 0.1

    elif t_norm > 0.2:  # Mid: å¹³è¡¡

        alpha = (t_norm - 0.2) / 0.4  # 0.6â†’0.2 æ˜ å°„åˆ° [1, 0]

        w1_base = 0.7 * alpha + 0.3 * (1-alpha)

        w2_base = 0.2 * alpha + 0.6 * (1-alpha)

        w3_base = 0.1

    else:  # Late: å¼ºè¯­ä¹‰

        w1_base = 0.3

        w2_base = 0.6

        w3_base = 0.1

    

    # æ›å…‰åº¦è°ƒåˆ¶

    w2 = w2_base * E_vi  # ä½å…‰æ—¶é™ä½ VI æƒé‡

    w3 = w3_base * (1 + 2*(1 - E_vi))  # ä½å…‰æ—¶å¢å¼ºæ–‡æœ¬å¼•å¯¼

    w1 = 1 - w2 - w3  # ä¿è¯å½’ä¸€åŒ–

    

    return w1, w2, w3

3.2 æƒé‡æ¼”åŒ–æ›²çº¿ï¼ˆè®ºæ–‡ç”¨ - éçº¿æ€§ç‰ˆï¼‰

import numpy as np



def compute_adaptive_weights_nonlinear(t, T, E_vi):

    """è®ºæ–‡å…¬å¼ç‰ˆæœ¬"""

    t_norm = t / T

    

    # IR æƒé‡ï¼šæŒ‡æ•°è¡°å‡

    w1 = 0.8 * np.exp(-3 * (1-t_norm)) + 0.1

    

    # VI æƒé‡ï¼šSigmoid è¿‡æ¸¡ Ã— æ›å…‰åº¦

    w2 = (1 / (1 + np.exp(-10*(t_norm - 0.5)))) * E_vi

    

    # Text æƒé‡ï¼šå‰æœŸ Ã— ä½å…‰

    w3 = 0.3 * (1 - E_vi) * (1 - t_norm)

    

    # å½’ä¸€åŒ–

    total = w1 + w2 + w3

    return w1/total, w2/total, w3/total

3ã€è®ºæ–‡å…¬å¼ï¼ˆLaTeX æ ¼å¼ï¼‰

3.1 æ ¸å¿ƒèåˆå…¬å¼

\subsection{Triple-Stream Hybrid Attention Fusion}



Given the query $\mathbf{Q} \in \mathbb{R}^{N \times d_k}$ derived from the current denoising state $\mathbf{z}_t$, we construct the fused keys and values by linearly combining three information streams:



\begin{equation}

\begin{aligned}

\mathbf{K}_{\text{fused}}^{(t)} &= w_1^{(t)} \mathbf{K}_{\text{ir}} + w_2^{(t)} \mathbf{K}_{\text{vi}} + w_3^{(t)} \mathbf{K}_{\text{txt}} \\

\mathbf{V}_{\text{fused}}^{(t)} &= w_1^{(t)} \mathbf{V}_{\text{ir}} + w_2^{(t)} \mathbf{V}_{\text{vi}} + w_3^{(t)} \mathbf{V}_{\text{txt}}

\end{aligned}

\end{equation}



where $\mathbf{K}_{\text{ir}}, \mathbf{V}_{\text{ir}}$ preserve thermal structural features, $\mathbf{K}_{\text{vi}}, \mathbf{V}_{\text{vi}}$ retain visible semantic information, and $\mathbf{K}_{\text{txt}}, \mathbf{V}_{\text{txt}}$ inject illumination priors from text embeddings.  The weights satisfy:



\begin{equation}

w_1^{(t)} + w_2^{(t)} + w_3^{(t)} = 1, \quad w_i \in [0, 1]

\end{equation}



The hybrid attention is then computed as:



\begin{equation}

\text{Attention}_{\text{hybrid}} = \text{softmax}\left(\frac{\mathbf{Q} \cdot \mathbf{K}_{\text{fused}}^{\top}}{\sqrt{d_k}}\right) \mathbf{V}_{\text{fused}}

\end{equation}

3.2 è‡ªé€‚åº”æƒé‡è°ƒåº¦

\subsection{Adaptive Temporal Weighting Schedule}



To adapt to varying illumination conditions, we design an exposure-aware scheduling strategy.  First, we quantify the illumination level of the visible image using: 



\begin{equation}

E_{\text{vi}} = \frac{1}{HW} \sum_{i,j} (0.299 R_{ij} + 0.587 G_{ij} + 0.114 B_{ij})

\end{equation}



The temporal weights are then modulated as:



\begin{equation}

\begin{aligned}

w_1^{(t)} &= \alpha_0 \cdot \exp\left(-\lambda \frac{T-t}{T}\right) + \beta_0 \\

w_2^{(t)} &= \frac{1}{1 + \exp\left(-k\left(\frac{t}{T} - 0.5\right)\right)} \cdot E_{\text{vi}} \\

w_3^{(t)} &= \gamma_0 \cdot (1 - E_{\text{vi}}) \cdot \left(1 - \frac{t}{T}\right)

\end{aligned}

\end{equation}



where $\alpha_0=0.8, \beta_0=0.1, \lambda=3, k=10, \gamma_0=0.3$ are empirically determined hyperparameters.  This ensures: 

\begin{itemize}

    \item \textbf{Early stage} ($T \to 0.6T$): High $w_1$ enforces IR structural guidance. 

    \item \textbf{Mid stage} ($0.6T \to 0.2T$): Smooth transition balancing structure and semantics.

    \item \textbf{Late stage} ($0.2T \to 0$): High $w_2$ recovers VI textural details.

\end{itemize}



For extremely dark scenes ($E_{\text{vi}} < 0.2$), $w_3$ is elevated throughout to leverage diffusion priors for hallucinating realistic illumination. 

3.3 ControlNet é›†æˆï¼ˆå¯é€‰æ¨¡å—ï¼‰

\subsection{Structure Constraint via ControlNet}



To further preserve infrared geometric fidelity, we optionally integrate ControlNet~\cite{zhang2023controlnet} with Canny edge maps:



\begin{equation}

\mathbf{E}_{\text{ir}} = \text{Canny}(\text{RGB2Gray}(\mathbf{I}_{\text{ir}}), \tau_{\text{low}}=50, \tau_{\text{high}}=150)

\end{equation}



The ControlNet residual features $\Delta \mathbf{F}_{\text{ctrl}}$ are injected into the decoder layers with a low weight $\alpha_{\text{ctrl}}=0.15$ to avoid over-constraining the generation: 



\begin{equation}

\begin{aligned}

\mathbf{K}_{\text{fused}} &\leftarrow \mathbf{K}_{\text{fused}} + \alpha_{\text{ctrl}} \Delta \mathbf{K}_{\text{ctrl}} \\

\mathbf{V}_{\text{fused}} &\leftarrow \mathbf{V}_{\text{fused}} + \alpha_{\text{ctrl}} \Delta \mathbf{V}_{\text{ctrl}}

\end{aligned}

\end{equation}



This module can be ablated ($\alpha_{\text{ctrl}}=0$) for faster inference without significant quality degradation in most scenarios. 

3.4 è‰²å½©ä¸€è‡´æ€§åå¤„ç†

\subsection{Color Consistency Post-processing}



To mitigate potential color distortions, we apply an adaptive color transfer in the YCbCr space:



\begin{equation}

\mathbf{I}_{\text{final}} = \text{YCbCr2RGB}(Y_{\text{gen}}, \beta Cb_{\text{gen}} + (1-\beta) Cb_{\text{vi}}, \beta Cr_{\text{gen}} + (1-\beta) Cr_{\text{vi}})

\end{equation}



where $\beta = \min(1, 0.5 + E_{\text{vi}})$ adaptively balances generated and original chrominance based on the exposure level. 

4ã€ç®—æ³•ä¼ªä»£ç ï¼ˆå†™å…¥è®ºæ–‡ Algorithm ç¯å¢ƒï¼‰

\begin{algorithm}[t]

\caption{LIT-Fusion:  Low-Light Infrared-Visible Fusion}

\label{alg:lit_fusion}

\begin{algorithmic}[1]

\REQUIRE Infrared image $\mathbf{I}_{\text{ir}}$, low-light visible image $\mathbf{I}_{\text{vi}}$

\ENSURE Fused image $\mathbf{I}_{\text{fusion}}$



\STATE \textbf{// Stage 1: Latent Initialization}

\STATE $\mathbf{E}_{\text{ir}} \gets \text{Canny}(\mathbf{I}_{\text{ir}})$

\STATE $\mathbf{R}, \mathbf{L} \gets \text{RetinexDecompose}(\mathbf{I}_{\text{vi}})$

\STATE $\mathbf{I}'_{\text{vi}} \gets \mathbf{R} \odot \mathbf{L}^{0.4}$ \COMMENT{Enhance illumination}

\STATE $E_{\text{vi}} \gets \text{ComputeExposure}(\mathbf{I}'_{\text{vi}})$ \COMMENT{Eq. 4}

\STATE $\mathbf{z}_{\text{ir}}, \mathbf{z}_{\text{vi}} \gets \text{DDPMInversion}(\mathbf{I}_{\text{ir}}, \mathbf{I}'_{\text{vi}}, E_{\text{vi}})$



\STATE \textbf{// Stage 2: Hybrid Attention Fusion}

\STATE $\mathbf{z}_t \gets [\mathbf{z}_{\text{vi}}, \mathbf{z}_{\text{vi}}, \mathbf{z}_{\text{ir}}]$ \COMMENT{Initialize 3 samples}

\FOR{$t = T-\text{skip\_steps}$ \TO $0$}

    \STATE $w_1, w_2, w_3 \gets \text{AdaptiveWeights}(t, T, E_{\text{vi}})$ \COMMENT{Eq. 5}

    \STATE $\mathbf{K}_{\text{fused}}, \mathbf{V}_{\text{fused}} \gets$ TripleStreamFusion($\mathbf{z}_t, w_1, w_2, w_3$) \COMMENT{Eq. 1}

    \IF{use\_controlnet}

        \STATE $\mathbf{K}_{\text{fused}} \gets \mathbf{K}_{\text{fused}} + 0.15 \cdot \text{ControlNet}(\mathbf{E}_{\text{ir}})$

    \ENDIF

    \STATE $\mathbf{z}_{t-1} \gets \text{DDIMStep}(\mathbf{z}_t, \mathbf{K}_{\text{fused}}, \mathbf{V}_{\text{fused}})$

\ENDFOR



\STATE \textbf{// Stage 3: Decode and Post-process}

\STATE $\mathbf{I}_{\text{gen}} \gets \text{VAEDecode}(\mathbf{z}_0)$

\STATE $\mathbf{I}_{\text{fusion}} \gets \text{ColorCorrection}(\mathbf{I}_{\text{gen}}, \mathbf{I}_{\text{vi}}, E_{\text{vi}})$ \COMMENT{Eq. 7}

\RETURN $\mathbf{I}_{\text{fusion}}$

\end{algorithmic}

\end{algorithm}

5ã€å®æ–½è·¯çº¿ï¼ˆMVP ä¼˜å…ˆï¼‰

Phase 1: MVP ç‰ˆæœ¬ï¼ˆDay 1-2ï¼‰

ç›®æ ‡ï¼šéªŒè¯æ ¸å¿ƒæœºåˆ¶ï¼Œä¸è¿½æ±‚å®Œç¾

# åŠŸèƒ½æ¸…å•

âœ… Retinex å¢å¼º + å¼•å¯¼åæ¼”

âœ… ä¸‰æµæ³¨æ„åŠ›èåˆï¼ˆçº¿æ€§æƒé‡ï¼‰

âœ… åŸºç¡€å»å™ªå¾ªç¯

âŒ æš‚ä¸åŠ  ControlNetï¼ˆweight=0ï¼‰

âŒ æš‚ä¸åšè‰²å½©åå¤„ç†



# éªŒè¯æŒ‡æ ‡

- èåˆå›¾æ˜¯å¦æ¯”åŸ VI å›¾äº®ï¼Ÿ

- æ˜¯å¦ä¿ç•™äº† IR çš„çƒ­ç›®æ ‡ï¼Ÿ

- æœ‰æ— æ˜æ˜¾ä¼ªå½±ï¼Ÿ

Phase 2: å®Œæ•´ç‰ˆæœ¬ï¼ˆDay 3-4ï¼‰

âœ… åŠ å…¥ ControlNetï¼ˆweight=0.15ï¼‰

âœ… å®ç°éçº¿æ€§æƒé‡ï¼ˆSigmoid/Expï¼‰

âœ… è‰²å½©åå¤„ç†

âœ… å®Œæ•´çš„ E_vi è‡ªé€‚åº”



# å¯¹æ¯”å®éªŒ

- MVP vs å®Œæ•´ç‰ˆ

- çº¿æ€§æƒé‡ vs éçº¿æ€§æƒé‡

- æœ‰æ—  ControlNet

Phase 3: è®ºæ–‡å®éªŒï¼ˆDay 5-7ï¼‰

âœ… LLVIP/MSRS/TNO æ•°æ®é›†æ‰¹é‡æµ‹è¯•

âœ… å®šé‡æŒ‡æ ‡ï¼šEN, MI, SF, VIF, Qabf

âœ… æ¶ˆèå®éªŒ

âœ… å¯è§†åŒ–å¯¹æ¯”